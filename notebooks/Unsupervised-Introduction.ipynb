{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a86fc7a",
   "metadata": {},
   "source": [
    "# General intro üëã\n",
    "\n",
    "Find the repo of this notebook at [autoencoders](https://github.com/arrrrrmin/autoencoders). \n",
    "\n",
    "**Reach out to me for question or ideas**:\n",
    "You can mail me, message me on mastodo [@arrrrrmin@chaos.social](https://chaos.social/@arrrrrmin) just open an issue in the [autoencoders](https://github.com/arrrrrmin/autoencoders)-repo. I also try to update this with more recent architectures. For exciting recommendations open an issue, maybe I'm able to add the architecture.\n",
    "\n",
    "**A few questions befor we start**:\n",
    "* Is there any experience in machine learning?\n",
    "* Can any imagine to write a thesis about this topic?\n",
    "* Has anyone tried any prompts with the hyped *GPT-4* or *Stable Diffusion* models?\n",
    "* What operating systems are running?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec04776-f29a-4891-83c5-a6989c2ea820",
   "metadata": {},
   "source": [
    "# Unsupervised Learning - An introduction\n",
    "\n",
    "Unsupervised learning is a type of machine learning where the algorithm learns patterns from unlabelled data. The goal of unsupervised learning is to discover hidden structures or patterns in the data. Techniques like clustering and dimensionality reduction are strongly related and user in unsupervised learning.\n",
    "\n",
    "While dimension reduction aims to make high-dimensional data manageable, clustering seeks to find the boundary for differentiation within the data. Common dimension reduction techniques are principal component analysis (PCA), based on linear transformation. The goal is to find the desired number of components by the eigenvectors of the covariance matrix of the data.\n",
    "Other methods include Uniform Manifold Approximation and Projection (UMAP) or t-distributed stochastic neighbor embedding (tSNE). Dimensionality reduction is often used in machine learning to visualize high dimensional features.\n",
    "\n",
    "Clustering on the other hand tries to optimize a number of cluster centroids on the data to describe it as groups. Clustering can be used to find shared characteristics in features or classify the data into classes (or groups). There is a number of different types of clustering methods. Connectivity models, centroid models, distribution models or density models. Well known clustering methods are KMeans, DBSCAN or MeanShift."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d714cf0b",
   "metadata": {},
   "source": [
    "## Notation\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|:--- | :--------------------------- |\n",
    "| $x_{i}$ | The $i$th input vector in the training set of size $1024$ |\n",
    "| $x'_{i}$ | Output vector corresponding to $i$ of size $1024$ |\n",
    "| $N$ | Number of samples in the training set |\n",
    "| $y_{i}$ | The $i$th target to compute loss function |\n",
    "| $\\hat{y}_{i}$ | The $i$th output corresponding to the $ith$ target |\n",
    "| $g_{\\phi}$ | Encoding module of varying architectures |\n",
    "| $f_{\\phi}$ | Decoding module reverse architecture of $g_{\\phi}$ |\n",
    "| $z$ | Latent coder of some tbd. dimension |\n",
    "| $\\alpha$ | Learning rate |\n",
    "| $b$ | Bias term, added to the linear transformation |\n",
    "| $A^{T}$ | Weight matrix $A$ in transposed form (swapping the dimensions) |\n",
    "| $a_{j}^{(l)}$ | The $j$th activation in the $(l)$th layer |\n",
    "| $p$ | Satisfied probability in range (0, 1) |\n",
    "| $\\hat{p}_{j}^{(l)}$ | The produced probability of $j$th activation in $(l)$th layer |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17833e9",
   "metadata": {},
   "source": [
    "## Literatur\n",
    "\n",
    "\n",
    "* [1] Geoffrey E. Hinton, and Ruslan R. Salakhutdinov. [‚ÄúReducing the dimensionality of data with neural networks.\"](https://www.science.org/doi/10.1126/science.1127647) Science 313.5786 (2006).\n",
    "* [2] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, Pierre-Antoine Manzagol. [‚ÄúExtracting and composing robust features with denoising autoencoders.\"](https://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdf) ICML, (2008).\n",
    "* [3] Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhutdinov. [‚ÄúImproving neural networks by preventing co-adaptation of feature detectors.‚Äù](https://arxiv.org/abs/1207.0580) arXiv preprint arXiv:1207.0580 (2012).\n",
    "* [4] Andrew Ng. [Sparse Autoencoder.](https://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf) Lecture notes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388ed3e7-61b0-4ae6-9b2c-3e9f38575eca",
   "metadata": {},
   "source": [
    "## On autoencoders\n",
    "\n",
    "The basic goal of the autoencoder is to find an accurate low dimensional representation of the input $x_{i}$, be learning to encode $g_{\\phi}$ and decode $f_{\\phi}$ the input to latent space (latent code) and reverse decode this latent representation back to the input. The training goal is to obtain good reconstructions from the original input. If that's the case the encoder ($g_{\\phi}$) can compress the input feature, while the decoder ($f_{\\phi}$) can reconstruct the input ($x'$). After training the encoder can be used to find good low dimensional representations from the input space, for classification or other tasks. Whereas the decoder can be used to generate outputs from low dimensional inputs (a fundamental concept to Generative Adverserial Networks (GANs)).\n",
    "\n",
    "\n",
    "![Simple autoencoder overview diagram](figures/SimpleAutoEncoder.png)\n",
    "\n",
    "As the image above suggestes the first network we want to observe is a simple 1-layer autoencoder. It works with a linear layer on the en- and decoding side. This is not a requirement, there are many other autoencoder architectures in the literature. Other known Autencoder layers are Convolutional or Long-Short-Term-Memory (LSTM) layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cd38a4",
   "metadata": {},
   "source": [
    "## Let's do some hands-on example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eac3bd9-f8c5-4960-bfcb-1b79b5eef9fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from autoencoders.data import MNISTDataModule\n",
    "from autoencoders.models.simple import SimpleEncoder, SimpleDecoder, SimpleBlottleneck\n",
    "from autoencoders.lightning_extension import AutoEncoder\n",
    "from autoencoders.scripts import run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff41c6b-c787-404b-a651-df15d069d9b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a batch size & training size (None means give me all)\n",
    "# Load the dataset and do some normalization and splitting (train, val, test)\n",
    "\n",
    "batch_size = 32\n",
    "training_size = None\n",
    "datamodule = MNISTDataModule(\"./dataset/\", batch_size, training_size)\n",
    "datamodule.prepare_data()\n",
    "datamodule.setup(stage=\"fit\")\n",
    "len(datamodule.mnist_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d255ccb-5295-4afa-8165-a2cff384b18e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image, label = datamodule.mnist_train[0]\n",
    "print(image)\n",
    "print(image.shape)\n",
    "print(image.max(), image.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bbba27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some jupyter magic\n",
    "%matplotlib inline\n",
    "\n",
    "# Change the number to see what other examples exist in the dataset\n",
    "image, _ = datamodule.mnist_train[54999]\n",
    "\n",
    "plt.imshow(image.squeeze(), cmap=\"binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c75caa1",
   "metadata": {},
   "source": [
    "Ok, we know our dataset contains images of 32 by 32 (originally thats 28, 28 but we padded (32, 32) to better fit memory bit ranges). From the autoencoder overview *diagram* above, we don't see how large the input size is and to what size latent code dimension $z$ it is reduced. We need to define this. Also we need to find a good learning rate ($\\alpha$). Finding a good learning rate for your model is always related to the data at hand. Obtaining a good learning rate is still a research topic.\n",
    "\n",
    "**Recall**: The learning rate can be roughly viewed as the amount of how much to apply the gradients to your weights. So to say how large step you do with each learning step, using the optimization algorithm. For a deeper dive see [\"On the importance of initialization and momentum in deep learning\"](https://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf) and [\"ADAM: A Method for Stochastic Optimization\"](https://arxiv.org/abs/1412.6980).\n",
    "\n",
    "**Shortly off road**: Randomly searching for *\"learning rate\"* in the title of arxiv.org papers already gives you an idea of how *unsolved* the question of a good learning rate is. \n",
    "\n",
    "Here the $0.01$ is fine. Might not be the best but it's good. In practise often this is found by simply trying it out and with a reasonable value people proceed and further optimize it in the end with hyperparameter searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2f2f10-b6c8-4428-bfde-692a5d6dba1c",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_shape = (1, 32, 32)\n",
    "latent_dim = 20\n",
    "alpha = 0.01\n",
    "encoder = SimpleEncoder(input_shape, latent_dim)\n",
    "decoder = SimpleDecoder(latent_dim, input_shape)\n",
    "bottleneck = SimpleBlottleneck(latent_dim)\n",
    "model = AutoEncoder(encoder, bottleneck, decoder, lr=alpha, recon_loss=\"rmse\", prevent_tb_logging=True)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63843cf9",
   "metadata": {},
   "source": [
    "Let's shorty tough on linear transformations. As mentioned above there are other ways to build autoencoders, but we'll focus on linear layers as they are the easiest to understand. The `nn.Linear` layer inside `SimpleEncoder` and `SimpleDecoder` applies $y=xA{T}+b$, where $A^{T}$ is the transposed weight matrix, $x$ is the input and $b$ the bias term. Our goal is to learn a good matrix of weights $A$, to accuratly encoder $x$ as $z$.\n",
    "We do the same in reverse for the decoder to reconstruct the latent code $z$ back to $x$'s size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5197a0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A very basic linear transformation example\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "inputs = torch.rand((32, 20))\n",
    "\n",
    "L1 = torch.nn.Linear(20, 18, bias=True)\n",
    "print(L1)\n",
    "print(\"L1.weight\", L1.weight.shape)\n",
    "print(\"L1.bias\", L1.bias.shape)\n",
    "\n",
    "x = L1(inputs)\n",
    "print(\"Sum of L1 output:\", x.sum().item())\n",
    "\n",
    "x = torch.mm(inputs, L1.weight.T) + L1.bias\n",
    "print(\"Sum of matrix multiplication:\", x.sum().item())\n",
    "\n",
    "# Another matrix multiplication example\n",
    "A1 = torch.randint(0, 10, (2, 3))\n",
    "A2 = torch.randint(0, 10, (3, 2))\n",
    "print(\"Weight matrix A1:\\n\", A1)\n",
    "print(\"Weight matrix A2:\\n\", A2)\n",
    "\n",
    "# torch.mm((n,k), (k,m)) -> (n,m)\n",
    "print(\"(n,k), (k,m)) -> (n,m)\")\n",
    "y = torch.mm(A1, A2)\n",
    "print(\"Shape of y mm(A1,A2)\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd86aae3",
   "metadata": {},
   "source": [
    "In order to find the weights that handle our problem well, we need to define a cost function, so we can iterate to a good enough model.\n",
    "\n",
    "Inside the `AutoEncoder` class there's a loss function defined. It's called *root mean square error* (RMSE).\n",
    "\n",
    "$L = \\sqrt{\\frac{1}{N}(\\hat{y}_{n} - x_{n})^{2}}$, where $x$ are the inputs and $\\hat{y}$ are the outputs of the decoder. Size we know our inputs are scaled between $0$ and $1$ we can also use *binary cross entropy* as loss function, but for now that fine.\n",
    "\n",
    "For the bce we'd need to sum over rows and cols of the image and compute the loss over every pixel: $L_{BCE} = \\frac{1}{N}\\sum_{1}^{N}\\sum_{1}^{h \\cdot c}BCE(\\hat{x}_{i}^{j}, x_{i}^{j})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919c627f-2aec-4b75-9cfa-9b2629130e33",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We need some jupyter magic to plot inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Increase the number of epochs for more accurate results\n",
    "# Note that the model will most likely converge around 20-35 epochs.\n",
    "epochs = 2\n",
    "logger = run.build_logger(\"simpleautoencoder\", datamodule_name=\"MNIST\", task=\"reconstruction\")\n",
    "run._train(model, logger, datamodule, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6297b9aa",
   "metadata": {},
   "source": [
    "## Deep Autoencoders\n",
    "\n",
    "Until now we only had single layer networks, for en- and decoding. We can do this with multiple and have a deep learning network on both sides. This will improve the network since we have not one matrix of weight to take care of the operations but as many as we want to have. But be aware, if we take to many we overfit, meaning we know only the training dataset and cannot handle any other - related but not the same - images.\n",
    "\n",
    "![Deep autoencoder overview diagram](figures/DeepAutoEncoder.png)\n",
    "\n",
    "Green layers are encoders, while blues are decoding layers. On the left of green layers we see input sizes. On the right of the decoding layer we see the respective output sizes of each layer and as shown in the diagram each layer has it's own $A^{T}$ and $b$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f832d6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencoders.models.deep import DeepEncoder, DeepDecoder\n",
    "\n",
    "# Play a little with latent dimension and depth.\n",
    "# If you just want faster results run it for 1 epoch and have a look.\n",
    "\n",
    "input_shape = (1, 32, 32)\n",
    "latent_dim = 20\n",
    "depth = 3\n",
    "alpha = 0.01\n",
    "encoder = DeepEncoder(input_shape, depth, latent_dim)\n",
    "decoder = DeepDecoder(latent_dim, depth, input_shape)\n",
    "bottleneck = SimpleBlottleneck(latent_dim)\n",
    "model = AutoEncoder(encoder, bottleneck, decoder, lr=alpha, noise_ratio=None, recon_loss=\"rmse\", prevent_tb_logging=True)\n",
    "# print(model)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "epochs = 2\n",
    "logger = run.build_logger(\"deepautoencoder\", datamodule_name=\"MNIST\", task=\"reconstruction\")\n",
    "run._train(model, logger, datamodule, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432eabb8",
   "metadata": {},
   "source": [
    "## Denoising Autoencoder\n",
    "\n",
    "The denoising autoencoder is the experiment to adjust the input with noise injection, so the image matricies recieved by the network are partially broken. The task for the network stays the same, but the comparison (loss calculations) are done on the original (non broken) inputs. The expectation is a denoising effect, as the model should learn to correct the noisy parts in the input image. \n",
    "\n",
    "![Denoising AutoEncoder](figures/DenoisingAutoEncoder.png)\n",
    "\n",
    "The image shows the proceedure, where we want to decode the noisily encoded latent code $x'$ and compare it not with $\\tilde{x}$ but with the original $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cab859",
   "metadata": {},
   "source": [
    "> A question about the intuition, where is the denoising aspect learned, if we use the same architecture and process as with the deepautoencoder run?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2821c2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from autoencoders.models.deep import DeepEncoder, DeepDecoder\n",
    "\n",
    "input_shape = (1, 32, 32)\n",
    "latent_dim = 20\n",
    "depth = 3\n",
    "alpha = 0.01\n",
    "noise_ratio = 0.25\n",
    "encoder = DeepEncoder(input_shape, depth, latent_dim)\n",
    "decoder = DeepDecoder(latent_dim, depth, input_shape)\n",
    "bottleneck = SimpleBlottleneck(latent_dim)\n",
    "model = AutoEncoder(encoder, bottleneck, decoder, lr=alpha, noise_ratio=noise_ratio, recon_loss=\"rmse\", prevent_tb_logging=True)\n",
    "# print(model)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "epochs = 2\n",
    "logger = run.build_logger(\"denoisingautoencoder\", datamodule_name=\"MNIST\", task=\"reconstruction\")\n",
    "run._train(model, logger, datamodule, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19818659",
   "metadata": {},
   "source": [
    "## Sparse Autoencoder\n",
    "\n",
    "In all the previous models we face a fundamental problem for neural architectures: \n",
    "> It's not interpretable\n",
    "\n",
    "The latent code we produce is not interpretable, meaning we cannot know which of the indecies of the latent vector is encoding which part of the feature. In vision tasks this a relativly easy to visualize problem, but imagine architectures for audio or vibration signals, this might be harder.  \n",
    "\n",
    "We'd have to at least cluster the latent code and compare it to known targets. The sparse encoder is injecting an additional loss function to force a sparse constraint on the latent code. The sparse constraint is a mathematical expectation formulated over the latent code:\n",
    "\n",
    "$\\hat{p}_{j}^{(l)} = \\frac{1}{N}\\sum_{n=1}^{N}[a_{j}^{(l)}(x^{i})] \\approx p$\n",
    "\n",
    "There are multiple ways to enforce this expectation. The additional loss function needs to penalize large $p$, in order to force the vector to sparsity. Sparsity wants small propabilities in each vector, for low representational meaning. \n",
    "\n",
    "> A sparse vector tries to apply meaning to the indecies of vectors, ideally every index not related to the representational task should be 0, thus the sparse vector often has relativly many zero or low values.\n",
    "\n",
    "These are the steps to enforce the sparsity constraint on the latent code:\n",
    "* Apply an additional activation (sigmoid) on latent code.\n",
    "* Average the batch dimension with size $N$.\n",
    "* Now we are left with a 1d latent code of size $z$ for each batch.\n",
    "* Apply the loss function to measure how well the above expectation is met.\n",
    "* Add the sparsity loss to the reconstruction loss\n",
    "* Backpropagate\n",
    "* Repeat üéâ\n",
    "\n",
    "To penalize large activations in the latent code activations, we'll use this loss: $L_{s} = \\sum_{j=1}^{J} \\log(1 + (\\hat{p}_{j}^{(l)})^{2})$\n",
    "\n",
    "This loss is a combination takes parts of the above mentioned square loss (square the probability) adding $1$ and taking the natural logarithm and summing it up. This way we can penalize high probabilities in the bottleneck activations.\n",
    "\n",
    "> Note the $1+$ in $\\log(1 + (\\hat{p}_{j}^{(l)})^{2})$ takes care of very low probabilities running towards zero not beeing negative. Negative losses can cause harm here, resulting in NaN values for backpropagation.\n",
    "\n",
    "![Sparse Auto Encoder](figures/SparseAutoEncoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109fcbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Let's see what the sparsity loss function does\n",
    "x = torch.rand((8,))\n",
    "sig_x = torch.sigmoid(x)\n",
    "\n",
    "def sparse_loss(inputs: torch.Tensor) -> torch.Tensor:\n",
    "    sig_x = torch.sigmoid(x)\n",
    "    print(\"Sigmoid activations\", sig_x)\n",
    "    # Uncomment to see the effect if we take the log of very small numbers \n",
    "    # sq_x = torch.pow(inputs, 2)\n",
    "    sq_x = 1 + torch.pow(inputs, 2)\n",
    "    print(\"Squared activations\", sq_x)\n",
    "    log_x = torch.log(sq_x)\n",
    "    print(\"Logarithm\", log_x)\n",
    "    sum_logs = log_x.sum()\n",
    "    print(\"Summed loss\", sum_logs)\n",
    "    return sum_logs\n",
    "\n",
    "print(\"Raw inputs\", x)\n",
    "sparse_loss(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54299b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencoders.models.deep import DeepEncoder, DeepDecoder\n",
    "from autoencoders.models.sparse import SparseBottleneck\n",
    "\n",
    "# Define a batch size & training size (None means give me all)\n",
    "# Load the dataset and do some normalization and splitting (train, val, test)\n",
    "\n",
    "input_shape = (1, 32, 32)\n",
    "latent_dim = 30\n",
    "depth = 2\n",
    "alpha = 0.01\n",
    "noise_ratio = None\n",
    "sparse_loss = \"log\"\n",
    "encoder = DeepEncoder(input_shape, depth, latent_dim)\n",
    "decoder = DeepDecoder(latent_dim, depth, input_shape)\n",
    "bottleneck = SparseBottleneck(latent_dim, sparse_loss)\n",
    "model = AutoEncoder(encoder, bottleneck, decoder, lr=alpha, noise_ratio=noise_ratio, recon_loss=\"bce\", prevent_tb_logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff87a022",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "# We need some jupyter magic to plot inline\n",
    "%matplotlib inline\n",
    "\n",
    "epochs = 2\n",
    "logger = run.build_logger(\"sparseautoencoder\", datamodule_name=\"MNIST\", task=\"reconstruction\")\n",
    "run._train(model, logger, datamodule, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c43135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Let's try to use the sparse latent code\n",
    "\n",
    "decoder.eval()  # Eval mode, stops the model from computing gradients\n",
    "\n",
    "x = torch.sigmoid(torch.rand((1, latent_dim)))\n",
    "# x = torch.tensor([[0.1, 0.1, 0.1, 1., 0.6868, 0.5492, 0.6735, 0.6416, 0.5190,\n",
    "#          0.7017, 0.6724, 0.5502, 0.7031, 0.6657, 0.6051, 0.5447, 0.7077, 0.6367,\n",
    "#          0.5936, 0.6066, 0.6646, 0.5281, 0.7120, 0.5786, 0.7286, 0.6224, 0.6174,\n",
    "#          0.7098, 0.6828, 0.5593]])\n",
    "gen_x = decoder(x).squeeze()\n",
    "print(\"Generated image shape\", gen_x.shape)\n",
    "plt.imshow(gen_x.detach().numpy(), cmap=\"binary\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
